---
title: "Data Wrangling Notebook"
author: "Juan Caballero"
date: "October 2024"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    number_sections: yes
subtitle: Data Science and Machine Learning 1516
editor_options:
  chunk_output_type: inline
bibliography: ./01_input_data/rmarkdown_files/art/literature.bib
---

```{r setup, include=FALSE}

# Set global chunk options
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")

# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Load required packages
required_packages <- c(
  "tidyverse",
  "openxlsx",
  "nycflights13",
  "broom",
  "here",
  "DT",
  "kableExtra",
  "countrycode",
  "styler"
)

# Install missing packages (if any)
installed_packages <- rownames(installed.packages())
for (p in required_packages) {
  if (!(p %in% installed_packages)) {
    install.packages(p)
  }
}

# Load the packages
invisible(lapply(required_packages, library, character.only = TRUE))

# Set up 'here' package
here::i_am("02_Data_wrangling_notebook.Rmd")

# Source any required functions (adjust the path as necessary)

# source(here("code", "functions.R"))

```

# Intro

Note that we are using the latest version of the Tidyverse package in this notebook. Please ensure you have updated to the most recent version to follow along without issues. Some of the syntax presented here won’t work with older versions (e.g., pivot_longer and pivot_wider). You will also need to install all the packages listed in the setup chunk above.

As mentioned before, the main resource for this part is R for Data Science by Hadley Wickham and Garrett Grolemund. You can find it for free at: <https://r4ds.hadley.nz/>

```{r, echo=FALSE, out.width="30%", fig.cap="Get it for free at: https://r4ds.had.co.nz/"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/r4datascience_cover.png")
```

# Project structure

Let’s first talk about how to set up projects. In this course, you will collaborate with your colleagues on a data science project. Here are some recommendations for setting up an R project for collaboration.

If I see something like this in your code, you will **lose** points:

```{r echo = T, results = 'hide',eval=FALSE}
# setwd("Y:/path/to/a/directory/on/my/computer/projectdir")
```

Never setwd! Always set relative paths. To make collaboration easier, we want to use **relative** paths wherever possible. This way, it does not matter where the R scripts and the data are located in the file system, and it does not matter whether the operating system uses drive letters (like Windows) or mount points (like macOS or Linux).

## Absolute vs relative Paths

Absolute and relative paths are two methods of specifying the location of files and directories in a file system.

***Absolute Path:***

-   Provides a specific location in the file system, independent of the current working directory. Often referred to as the full path. Example: If your project is in `C:\rproject1` and pictures are in `C:\rproject1\pics`, the absolute path is `C:\rproject1\pics`.

-   Commonly used to link to external websites or resources outside the current domain.

***Relative Path:***

-   Specifies a location using the current directory as a reference point. Known as a non-absolute path. Example: If your project is in `C:\rproject1` and pictures are in `C:\rproject1\pics`, the relative path is `.\pics`. Typically used for resources within the same domain or project structure.

-   Using the appropriate path type ensures efficient file access and enhances the portability and organization of your projects.

## How to use relative Paths in R

To create a point of reference with an .Rproj file, we create a R project via the R-Studio GUI:

```{r, echo=FALSE, out.width="100%",fig.align="center"}

knitr::include_graphics("./01_input_data/rmarkdown_files/art/create_project.PNG")
```

These steps will create a project directory and a .Rproj file inside the directory. All file paths can now be entered "relative" to the directory where the .Rproj file resides by entering a "." to indicate the "root" directory of the R-project.

```{r, echo=FALSE, out.width="70%",fig.align="center"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/rproject.png")
```

### Navigating File Paths in R

As projects grow in complexity, with multiple directories and sub-directories, managing file paths becomes a high importance task. Different operating systems have their own conventions for file paths, and manually constructing these paths can lead to errors. Thankfully, R provides tools that make this process more streamlined.

```{r echo = T, results = 'hide',eval=FALSE}

a_data_frame <- read_csv("data_file.csv")

another_data_frame <- read_csv(file.path(".", "sub_directory", "another_data_file.csv"))

yet_another_data_frame <- read_csv(here("sub_directory", "yet_another_data_file.csv"))

```

#### file.path Function

The file.path() function in R is designed to construct platform-independent file paths. It takes components of a file path and combines them, ensuring the correct file separator for the operating system is used.

-   ***Purpose:***

    -   The file.path function in R is designed to construct platform-independent file paths. It takes components of a file path and combines them, ensuring the correct file separator for the operating system is used.

-   ***Benefits:***

    -   **Platform Independence:** Adjusts for file path conventions of different operating systems.

    -   **Structured Paths:** Provides a structured way to combine path components.

    -   **Reproducibility:** Makes R scripts more portable.
    
#### Here Package

The here package simplifies the process of building file paths. It constructs file paths relative to your project’s top-level directory, typically identified by an .Rproj file or a .git folder.

-   **Purpose:**

    -   It constructs file paths based on your project's top-level directory, typically identified by an .Rproj file or a .git folder.

-   **Benefits:**

    -   Platform Independence: Handles file path differences between operating systems.

    -   Simplified Paths: Use shorter, relative paths without worrying about the working directory.

    -   Reproducibility: Ensures consistency across different environments.

```{r echo = T, results = 'hide',eval=T}
here("sub_directory","another_data_file.csv")

```

## Small to medium projects

If you are working alone on a small to medium-sized project (like your master’s thesis), a project structure like this is recommended:

```{r, echo=FALSE, out.width="70%",fig.align="center"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/project_stucture.PNG")
```

-   The `00_code` folder is where you save the R-scripts
-   The `01_input_data` folder is where you all your input data resides
-   The `03_output folder` is where you save all the outputs

Careful: if your data is larger than \> 100MB, consider storing it outside of Git repositories or using tools like Git Large File Storage (LFS).

## Medium to Large Projects

For larger projects or collaborative work, especially when data is larger than 100 MB or confidential, it’s advisable to separate the code from the data. You can use a version control system like GitHub for code and a cloud storage solution like Dropbox or Google Drive for data.

```{r, echo=FALSE, out.width="70%",fig.align="center"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/dropbox_folders.PNG")
```

-   The 01_inputs folder is where you all your input data resides
-   The 02_intermediary_data is where all the intermediary steps are saved
-   The 03_output folder is where you save all the final outputs of the model

Of course you can have more sub-directories in this structure to make things even more organized.


## File Naming for Organization, Cooperation and Reproducibility

Reproducibility is fundamental in data science. Clear naming conventions and organized structures are essential for ensuring consistent and repeatable results.

1.  **Script Ordering:** Prefix scripts with increasing numbers to dictate their execution order, e.g., "001_data_cleaning.R" precedes "002_data_analysis.R".

2.  **Centralize Functions:** Store recurring functions in \_\_functions.R. This promotes easier updates and reduces redundancy.

3.  **Manage Dependencies:** Keep all package loadings in \_\_dependencies.R. This ensures a consistent environment across scripts and simplifies package management.

4.  **Descriptive Script Names:** Names should reflect the script's purpose, e.g., "00x_time_series_forecasting.R" over "00x_analysis1.R".

5.  **Master Script:** Use a master script to run all scripts in sequence, serving as both a guide and documentation.

When saving processed data, differentiate it from raw data by storing it in the `02_intermediary_data` folder.

### Naming Recommendations:

1.  **Script Reference:** Match data file prefixes with their generating script, e.g., data from "001_prepare_remittances_matrix.R" should be "001_descriptive_filename.rds".

2.  **Descriptive File Names:** Ensure file names reflect the data's content or stage.

3.  **Temporary File Names:** If unsure about script order, use temporary names like "00x_remittances_raw.rds", updating them later as needed.

4.  **Consistency:** Use underscores over spaces and maintain lowercase for cross-OS compatibility.

### Organizing Dependencies and Functions:

```{r, echo=FALSE, out.width="100%",fig.align="center"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/dependencies.PNG")
```

1.  **Dependencies Script:** Store all package dependencies in `__dependencies.R` in the 00_code directory. Source this at the start of each script for consistency.

2.  **Functions Script:** Save custom functions in `__functions.R` in the `00_code directory`. If reused across projects, consider creating a custom package for them.

Sourcing files in R means loading the content of a script into the current session, executing all its commands. This is particularly useful for organizing and reusing code, such as loading required packages or custom functions, ensuring consistency across different scripts in a project.

Here's an example of how you might source the `__dependencies.R` and `__functions.R` scripts:

```{r echo = T, results = 'hide',eval=FALSE}

# Source the dependencies and functions scripts
source(here("01_code", "dependencies.R"))
source(here("01_code", "functions.R"))

# Now all the packages and custom functions from those scripts are available for use in the current session.

```

By following these guidelines, you streamline your R projects, ensuring clarity, reproducibility, and efficient collaboration.

### Visual Representation of Naming Convention:

| **Script Name**                  | **Corresponding Data File**   |
|----------------------------------|-------------------------------|
| 001_prepare_remittances_matrix.R | 001_remittances_processed.rds |
| 002_analyze_demographics.R       | 002_demographics_analysis.rds |

By adhering to a clear naming convention, you not only make your project more organized but also ensure that anyone revisiting the project, including your future self, can easily navigate and understand the data processing stages.

# Style Guides

A style guide is a set of conventions and standards for writing and formatting code. In the R community, the [Tidyverse Style Guide](https://style.tidyverse.org/ "Tidyverse Style Guide") stands out as a beacon of best practices. Developed by Hadley Wickham and the Tidyverse team, this guide provides clear guidelines on various aspects of R coding.

To ensure that the code remains consistent with the style guide, it’s beneficial to use a code styler before committing changes to a repository. The styler package can be used to style code according to the Tidyverse Style Guide. 

```{r echo = T, results = 'hide',eval=FALSE}

# Install styler if not already installed
install.packages("styler")

# Style a script
styler::style_file("script.R")

```


-   **Style Guides:** A style guide is a set of conventions and standards for writing and formatting code. It ensures that code, especially in collaborative projects, is consistent, readable, and maintainable. When everyone follows the same style, it's easier to understand the codebase, track changes, and identify errors.

-   **Consistency:** With multiple contributors to a codebase, a style guide ensures that everyone writes code in a similar manner. This makes the code look as if it was written by a single person, which aids in readability.

-   **Error Reduction:** Consistent code can reduce the likelihood of errors. For instance, if a style guide enforces a particular way of handling null or undefined values, it can prevent null reference errors.

-   **Code Reviews:** A consistent style simplifies code reviews. Reviewers can focus on the logic and functionality of the code rather than getting distracted by formatting discrepancies.

# Importing Data

```{r, echo=FALSE, out.width="60%", fig.cap="Phases of a typical data science project (R for data science)"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/data-science-wrangle.png")
```

## CSV Files

For social science related work the most important file formats are (still) CSV files and excel files.

CSV (Comma-Separated Values) files are a common format for tabular data. They are plain text files, which makes them simple but not very efficient in terms of storage.

```{r, echo=FALSE, out.width="50%"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/csv_file.PNG")
```

CSV files come in two flavors:

-   International files (use commas as separator)
-   European files (use points as separator)

Use **read_csv()** from the readr package (part of the Tidyverse) to read CSV files. For CSV files that use semicolons as separators (common in Europe), use **read_csv2()**.

Lets now read in the file "measles_lev1.csv" in the data folder (data file sourced from [here](https://github.com/royfrancis/royfrancis.github.io/blob/master/a-guide-to-elegant-tiled-heatmaps-in-r/measles_lev1.csv)). As you can see this CSV file isn't read in correctly, because the first two lines are meta-data not the column names:

```{r echo=TRUE, message=FALSE, warning=FALSE}


measles <- read_csv(
  here(
    "01_input_data",
    "measles_lev1.csv"
  )
)

head(measles)

```

Even if it is tempting:

> **Do not edit the CSV file manually to remove unwanted rows or columns. Instead, handle this within your R script to maintain reproducibility.**!

We want to make every data wrangling step **reproducible**. Klicking around in Excel is not easily reproducible. If you have to re-run all of your code at some point (and this will happen) you **WILL** forget crucial data-wrangling steps.

> **EACH** data-wrangling step **should be done in R** and the **source data should be left UNTOUCHED**.

Trust me, it is possible.

In our case we simply have to tell the package to skip the first two rows:

```{r echo=TRUE, message=FALSE, warning=FALSE}
measles <- readr::read_csv(
  here(
    "01_input_data",
    "measles_lev1.csv"
  ),
  skip = 2
)

head(measles)

```

## Excel files

Excel files are widely used in business and research. The readxl and openxlsx packages are excellent for reading and writing Excel files.

Fortunately the underlying data-format has been standardized by Microsoft a few years ago. It is now quite easy to generate and read in excel files.

## A real world example:

We now want to read in the [billateral remittances matrix](https://www.worldbank.org/en/topic/migrationremittancesdiasporaissues/brief/migration-remittances-data) from the World Bank. It shows how much money migrants send back home to their families per year for all countries of the world.

Again the data isn't read in correctly because the first line are not the column names and there are meta data entries at the bottom of the matrix.

```{r echo=TRUE, message=FALSE, warning=FALSE}
remittances <- readxl::read_xlsx(
  here(
    "01_input_data",
    "Worldbank",
    "Remittance_matrix",
    "bilateralremittancematrix2017Apr2018.xlsx"
  )
)


remittances <- openxlsx::read.xlsx(
  here(
   "01_input_data",
    "Worldbank",
    "Remittance_matrix",
    "bilateralremittancematrix2017Apr2018.xlsx"
  )
)
```

```{r, echo=FALSE, out.width="100%",fig.align="center"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/remit_matrix1.PNG")
```

To solve this we open the file in excel, look at the rows you want to read in and enter them into the package:

```{r echo=TRUE, message=FALSE, warning=FALSE}

remittances <- openxlsx::read.xlsx(
  here(
    "01_input_data",
    "Worldbank",
    "Remittance_matrix",
    "bilateralremittancematrix2017Apr2018.xlsx"
  ),
  rows = 2:217
)

remittances <- readxl::read_xlsx(
  here(
    "01_input_data",
    "Worldbank",
    "Remittance_matrix",
    "bilateralremittancematrix2017Apr2018.xlsx"
  ),
  skip = 1,
  n_max = 216
)

```

# Saving Data

## Saving Data the Right Way

Avoid using **save()** and .RData files for saving data, as they can overwrite existing objects in your environment without warning.

Use **saveRDS()** and **readRDS()** instead. They allow you to save and load single R objects and specify the object name upon loading.


```{r echo=TRUE, message=FALSE, warning=FALSE}

# Let's save the remittances object 
saveRDS(
  remittances, 
  file = file.path(
    here(
      "01_input_data",
      "01_remittances_raw.rds"
      )
    )
  )


```

When loading the object back into the environment, the RDS format requires you to specify the name of the object. This might seem like an extra step, but it ensures clarity and consistency in your code, making it easier to maintain in the long run.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Load the remittances object 
remittances_raw <- readRDS(file = file.path(here("01_input_data", "01_remittances_raw.rds")))

```

### Other Popular File Formats

While RDS is a staple in R programming, there are several other file formats that are popular and efficient for specific use-cases:

1.  **FST**: An ultra-fast serialization format, ideal for large datasets. It offers both speed and compression.

2.  **Arrow**: Apache Arrow provides columnar storage, beneficial for analytics. It ensures compatibility across different programming languages.

3.  **Pickle**: Popular in the Python community, it's a way to serialize and deserialize Python objects. While it's not native to R, there are tools to work with Pickle files in R.

4.  **Feather**: Developed by the creators of R and Python, Feather provides a fast, lightweight file format that's especially useful for data frames. It ensures compatibility between R and Python.

5.  **Parquet**: An open-source columnar storage format, Parquet is optimized for analytics and compatible with a variety of data processing tools.

6.  **qst**: A fast and efficient data storage format, qst offers high compression ratios and is designed for use with the **`qs`** package in R.

7.  **JSON**: A lightweight data-interchange format, JSON is easy for humans to read and write and easy for machines to parse and generate. It's widely used for asynchronous browser/server communication.

# Filtering data & pipes

Lets have a closer look at the remittances matrix. As you can see the first column name is too long and too complicated. And we want to drop the row for WORLD and the column for WORLD.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# We only show the first 5 column names, because there are so many of them.
colnames(remittances)[1:5]

```

Lets now:

-   Rename the first column to a more sensible name.
-   Exclude the row for "WORLD"
-   Exclude the column for "WORLD"

## The wrong way

There are many ways of achieving this. I will show you two of them using Base-R syntax and chaining commands together using magrittr.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# create tible from data frame
cleaned_remittances0 <- as_tibble(remittances)

# First we rename the first column of the remittances object
# and save this into a new  object
cleaned_remittances1 <- rename(cleaned_remittances0, name_o = 1)

# then we filter this new object to exclude the
# row containing the string "WORLD" in the first column

cleaned_remittances2 <- filter(cleaned_remittances1, name_o != "WORLD")

# Then we exclude the column "WORLD"

cleaned_remittances3 <- select(cleaned_remittances2, -WORLD)

```

We just created four new objects, using up precious RAM, just because we did a few simple wrangling steps. We could of course also overwrite the original "remittances" object in each wrangling step, but this is **HARD** to debug, trust me! You will forget which steps already have been executed and the whole overwriting objects in each step aproach is a big mess.


## Using pipes

Avoid creating multiple intermediate objects when transforming data. Use the pipe operator %\>% to chain commands.
Tip: You can insert a pipe it via the R-Studio shortcut Ctrl + Shift + M (Windows) (Command for Mac)

This makes the datawrangling flow much more intuitive, more readable and more easy to debug, although it takes some time to get used to. It is tempting to chain endless commands together but as a rule of thumb you should stop after 10 - 15 wrangling steps and create a new object.

In my opinion the embrace of piping is one of the main reasons why the tidyverse shines and is so widely adapted. It seems as if the R-consortium agrees with this! Since R Version 4.10 Base-R (yes!) also features a native pipe operator: \|\> 

```{r echo=TRUE, message=FALSE, warning=FALSE}

# First we rename the first column of the remittances object
# and save this into a new  object

cleaned_remittances <- remittances %>%
  # create tible from data frame
  as_tibble() %>%
  # Rename the first column
  rename(name_o = 1) %>%
  # Filter rows to exclude the string "WOLRD"
  filter(name_o != "WORLD") %>%
  # Drop the column named "WORLD"
  select(-WORLD)

# in order to use the native pipe operator
cleaned_remittances <- remittances |>
  as_tibble() |>
  rename(name_o = 1) |>
  filter(name_o != "WORLD") |>
  select(-WORLD)

```

Another handy operator is the %in% operator. Use %in% to filter for multiple values:

```{r echo=TRUE, message=FALSE, warning=FALSE}

# First we rename the first column of the remittances object
# and save this into a new  object
cleaned_remittances_small <- remittances %>%
  # Rename the first column
  rename(name_o = 1) %>%
  # Filter rows only for Austria and Germany
  filter(name_o %in% c("Austria", "Germany")) %>%
  # Select cols for Austria and Germany
  select(c("name_o", "Austria", "Germany"))

cleaned_remittances_small
```

The filter command can be used with a bunch of other boolean operations. Heres a handy graphic for the syntax, taken from [R for datascience](https://r4ds.had.co.nz/transform.html). Please use the respective chapter as reference

This part is inspired by the "tidy data" chapter in [R for datascience](https://r4ds.had.co.nz/transform.html) when you need to filter your data sets for your homework assignments.

```{r, echo=FALSE, out.width="60%",fig.align="center"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/transform-logical.png")
```

# Tidy Data and Creating Variables

Please read the chapter if you don't get what we are talking about here. We will go beyond the toy datasets presented there and look at some real-world datasets.

## Wide vs. Long data

As you can see below the same data can be represented in different tabular structures.

```{r, echo=FALSE, out.width="60%", fig.cap="source: http://jonathansoma.com/tutorials/d3/wide-vs-long-data/"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/wide_vs_long.PNG")
```

It is often neccessary to reshape (pivot) the data from wide to long or vice versa. Especially time series excel files are represented in wide format (for instance Eurostat time series). R packages usually prefer "long" data, this format is especially important for using the tidyverse. The reason behind this is that is is much easier/efficient to work with "vectorized" functions. This is how R can flex its muscles. Functions usually operate on all elements of a vector. It is not necessary to loop row-wise through your data using an index and act on each element of the respective vector.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# two examples of vectorization

c(1, 2, 3) + c(1, 2, 3)

dat <- data.frame(
  x = c(1, 2, 3),
  y = c(1, 2, 3)
) %>%
  mutate(z = x + y)

head(dat)


# suboptimal and why to use the tidyverse
# look how much code we need and how hard it is to read and understand...

for (i in 1:nrow(dat)) {
  dat$z2[i] <- dat$x[i] + dat$y[i]
}

dat$z3 <- dat$x + dat$y

dat

```

## Rules for tidy data

1.  Each variable must have its own column.
2.  Each observation must have its own row.
3.  Each value must have its own cell.

```{r, echo=FALSE, out.width="100%", fig.cap="source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/tidy-1.png")
```

## Which of these toy datasets are tidy?

```{r echo=TRUE, message=FALSE, warning=FALSE}

table1  

table2  

table3  
```

## Creating Variables: mutate()

But why should we bother reshaping our data?

Because we can use vectors (columns) as arguments in our function. Here an example what I mean by this.

The mutate command is used to create new columns in dplyr, you can simply use the column names of the original dataset without bothering naming the original data frame if it is used in conjunction with the magrittr pipe %\>%. This is also called "lazy evaluation" it is super useful for data wrangling but can become a bit cumbersome when programming functions.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# The base Base R way is also "VECTORIZED" you can do an easy cell by cell operation by using whole column vectors

table1$cases_per_capita <- table1$cases / table1$population

table1

# the Base R syntax is a bit redundant because you have to mention the 
# object every time to identify the vectors for the operations

# The TidyR syntax is a lot more succinct for the same operation
# Here the mutate() function is used to create a new column in the table1 object

# You can use virtually every function that accepts vectors as arguments.
# If you use basic math the operations are element wise

table1 %>%
  mutate(cases_per_capita = cases / population)


# It is super easy is to string together a chain of data wrangling
# and visualization steps without bothering to save intermediary object in RAM
# This is useful for one-off tables or quick visualizations

table1 %>%
  mutate(cases_per_capita = cases / population) %>%
  ggplot() +
  aes(x = factor(year), y = cases_per_capita, fill = country) +
  geom_col(position = "dodge") 

```

## Pivoting

### Wide to long

Time series are often in "wide" format. To reshape the data into "long" format we use the "pivot_longer()" function:

```{r echo=TRUE, message=FALSE, warning=FALSE}

table4a


table4a %>%
  pivot_longer(
    # select which columns should be pivoted
    cols = c(`1999`, `2000`),

    # how should the variable for the cols be named?
    names_to = "year",

    # how should the variable for the cells of the matrix be named?
    values_to = "cases"
  )

# slightly less cumbersome syntax:
table4a %>%
  pivot_longer(
    # select which columns should be pivoted by excluding the country column
    cols = c(-country),

    # how should the variable for the cols be named?
    names_to = "year",

    # how should the variable for the cells of the matrix be named?
    values_to = "cases"
  )

```

```{r, echo=FALSE, out.width="100%", fig.cap="Wide to long, source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/tidy-9.png")
```

### Long to wide

The opposite direction is done by the "pivot_wider()" function.

```{r echo=TRUE, message=FALSE, warning=FALSE}
table2


table2 %>%
  pivot_wider(

    # from which variable should the new column names be taken from
    names_from = type,

    # from which variable should the values be taken from
    values_from = count
  )

```

```{r, echo=FALSE, out.width="100%", fig.cap="Long to wide, source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/tidy-8.png")
```

## A real World Example

Is the remittances matrix tidy?

**No!** The columns represent the same variable. To make it tidy we have to pivot it from "wide" to "long" format.

We realize that several columns aren't numeric variables but character variables, this will lead to troubles while pivoting. Reading in the excel table did not work perfectly.

1.  Lets first turn all the column names that aren't numerical into a vector:

```{r echo=TRUE, message=FALSE, warning=FALSE}
character_cols <- remittances %>%
  # select all columns that are character variables
  select_if(is.character) %>%
  # drop the first column because we don't need it
  select(-1) %>%
  # create a vector containing the column names
  colnames()

character_cols

```

We now include two more steps into our data wrangling chain.

2.  We convert the character columns to numerical columns and
3.  pivot the remittances matrix into "long" format.

Now every variable has its own column.

Note that this results in a **MUCH** larger dataset than the original matrix. Storing data in this way is not efficient but it has huge benefits for running models or doing calculations with it.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# First we rename the first column of the remittances object
# and save this into a new  object
cleaned_remittances <- remittances %>%
  # Rename the first column
  rename(name_o = 1) %>%
  # Filter rows to exclude the string "WORLD"
  filter(name_o != "WORLD") %>%
  # Drop the column named "WORLD"
  select(-WORLD) %>%
  # We now convert all character cols to numeric
  mutate_at(character_cols, as.numeric) %>%
  # Pivoting
  pivot_longer(
    # new variable name for the col names
    names_to = "name_d",
    # new variable name for data in the cells
    values_to = "remit",
    # we want to pivot all the columns BUT the first
    cols = -name_o
  ) %>%
  # Drop all missing values from remit column
  filter(!is.na(remit))


dim(remittances)

dim(cleaned_remittances)

cleaned_remittances

```

# Joins

The examples in this part are taken almost in verbatim from the excellent "relational data" chapter in [R for datascience](https://r4ds.had.co.nz/relational-data.html). I strongly suggest to read the chapter if you don't get what we are talking about here. We will also go beyond the toy datasets presented in the book and look at some real-world datasets.

Lets have a look at the data in the nycflights13 library and think about Keys.

Keys are variables that connect pairs of tables.

-   Imagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables would you need to combine?

```{r, echo=FALSE, out.width="100%", fig.cap="Flights data relationship, source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/relational-nycflights.png")
```

```{r echo=TRUE, message=FALSE, warning=FALSE}

airlines 

airports 

planes 

weather 

flights

```

## Keys

-   "A **primary key** uniquely identifies an observation in its own table. For example, planes\$tailnum is a primary key because it uniquely identifies each plane in the planes table."

-   "A **foreign key** uniquely identifies an observation in another table. For example, flights\$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane."

Source: [R for datascience](https://r4ds.had.co.nz/relational-data.html)

Is the variable tailnum in the planes object a primary key?

If yes every observation should have its unique key. There are two ways to check that

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Base R

planes %>%
  View()

length(unique(planes$tailnum)) == nrow(planes)

# Tidy
planes %>%
  count(tailnum) %>%
  filter(n > 1)

```

With the Base-R approach above is not possible to check if a combination of variables uniquely identifies the observations (unless you create a variable first). Here the count package shines (I strongly recommend to use it often to explore or even summarise your data). You can usually also just click on the dataset on R studio.

Here an example with the weather object from nycflights13 package:

-   How would you solve this issue if you needed a dataset with a primary key?

```{r echo=TRUE, message=FALSE, warning=FALSE}

weather %>%
  View()

weather %>%
  count(year, month, day, hour, origin) %>%
  filter(n > 1)

weather %>%
  filter(year == 2013 & month == 11 & day == 3 & hour == 1)


```

## Mutating Joins

These are by far the most common joins you will use in your day to day work. The goal here is to create new columns from a new data set in your source dataset. I use "left joins" most of the time but there are many use cases for the other joins.

# Types of joins 
	•	Inner Join: Keeps only observations that match in both tables.
	•	Left Join: Keeps all observations in the left table.
	•	Right Join: Keeps all observations in the right table.
	•	Full Join: Keeps all observations in both tables.

The set operations underlying the mutating joins can be summarised with these venn diagrams.

```{r, echo=FALSE, out.width="50%", fig.cap="Joins as set operations, source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/join-venn.png")

```

But the alternative graphical representation from R for datascience below might be more useful for you because it shows what actually happens in your tables.

```{r, echo=F, out.width="20%", fig.cap="Data sets for join demonstration, source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/join-setup.png")
```

```{r echo=TRUE, message=FALSE, warning=FALSE}

x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     3, "x3"
)
y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2",
     4, "y3"
)

```

### Inner Join

The simplest type of join is the inner join. An inner join matches pairs of observations whenever their keys are equal:

```{r, echo=FALSE, out.width="50%", fig.cap="Inner join, source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/join-inner.png")

```

```{r echo=TRUE, message=FALSE, warning=FALSE}

x %>% 
  inner_join(y, by = "key")


```

### Outer Joins

An inner join keeps observations that appear in both tables. An outer join keeps observations that appear in at least one of the tables. There are three types of outer joins:

-   A left join keeps all observations in x.
-   A right join keeps all observations in y.
-   A full join keeps all observations in x and y.

You should use left joins as often as possible.

```{r, echo=FALSE, out.width="50%", fig.cap="Outer joins, source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/join-outer.png")

```

-   The default, by = NULL, uses all variables that appear in both tables, the so called natural join. For example, the flights and weather tables match on their common variables: year, month, day, hour and origin.

```{r echo=TRUE, message=FALSE, warning=FALSE}

flights2 <- flights %>% 
  select(year:day, hour, origin, dest, tailnum, carrier)

flights2


flights2 %>% 
  left_join(weather)

```

-   A character vector, by = "x". This is like a natural join, but uses only some of the common variables. For example, flights and planes have year variables, but they mean different things so we only want to join by tailnum.

```{r echo=TRUE, message=FALSE, warning=FALSE}

flights2 %>% 
  left_join(planes, by = "tailnum")


flights2 %>%
  glimpse()

planes %>%
  glimpse()


flights2 %>% 
  left_join(planes %>%
              select(-year), 
            by = "tailnum")


```

-   A named character vector: by = c("a" = "b"). This will match variable a in table x to variable b in table y. The variables from x will be used in the output.

```{r echo=TRUE, message=FALSE, warning=FALSE}

flights2
airports

flights2 %>% 
  left_join(airports, c("dest" = "faa")) %>% 
  left_join(airports,c("origin" = "faa")) %>%
  colnames()

```

## Filtering Joins

Filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables. There are two types:

-   semi_join(x, y) keeps all observations in x that have a match in y.
-   anti_join(x, y) drops all observations in x that have a match in y.

### Semi Join

```{r, echo=FALSE, out.width="50%", fig.cap="Outer joins, source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/join-semi.png")

```

Semi-joins are useful for matching filtered summary tables back to the original rows. For example, imagine you've found the top ten most popular destinations:

```{r echo=TRUE, message=FALSE, warning=FALSE}

top_dest <- flights %>%
  count(dest, sort = TRUE) %>%
  head(10)

top_dest


flights %>% 
  semi_join(top_dest)


flights %>%
  filter(dest==top_dest$dest)


```

### Anti join

The inverse of a semi-join is an anti-join. An anti-join keeps the rows that don't have a match:

```{r, echo=FALSE, out.width="50%", fig.cap="Outer joins, source: https://r4ds.had.co.nz"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/join-anti.png")

```

Anti-joins are useful for diagnosing join mismatches. For example, when connecting flights and planes, you might be interested to know that there are many flights that don't have a match in planes:

```{r echo=TRUE, message=FALSE, warning=FALSE}

flights %>%
  anti_join(planes, by = "tailnum") %>%
  count(tailnum, sort = TRUE)

```

## A real world example

Suppose we want to test the hypothesis that remittance flows can be explained by a basic gravity specification as suggested by @McCracken2017 and @Ahmed2016.

$$\ln R_{ij} = \beta_1 \ln GDP_{i} +  \beta_2 \ln GDP_{j} + \delta \ln d_{ij} + \sum_{k=1}^p \theta_k x_{k,ij} + \nu_{ij}$$

where (logged) remittances from country $i$ to country $j$ ($R_{ij}$) are assumed to depend on the GDP level of the origin and destination country ($GDP_{i}$ and $GDP_{j}$),on mobility and transaction costs, which are proxies of the distance between the two countries ($d_{ij}$) and other factors, captured by the $x_{k,ij}$ covariates

To test this hypothesis we need to join a distance data set as well as the origin GDP per capita and the destination GDP per capita to the cleaned remittances data set we created before.

-   The "dyadic" distance dataset between countries is sourced from the [CEPII GeoDist database](http://www.cepii.fr/cepii/en/bdd_modele/presentation.asp?id=6)

-   The GDP per capita dataset is sourced from the [World Development Indicators](http://datatopics.worldbank.org/world-development-indicators/)

### Reading in and cleaning GDP Data

As you can see the CSV file the World Bank provided can't be parsed correctly because of a missing column name.

```{r, echo=T}

indicators <- c("NY.GDP.PCAP.CD") # GDP per capita (current US$))

unzip(here("01_Data_Wrangling", "01_input_data", "Worldbank", "World_Development_Indicators", "WDI_csv.zip"),
  files = "WDIData.csv",
  # exdir="./data/rds",
  exdir = tempdir(),
  junkpaths = TRUE
)


#wdi <- read_csv(file.path(file.path(tempdir(), "WDIData.csv")))
wdi <- readr::read_csv(here("01_input_data", "Worldbank", "World_Development_Indicators", "WDIData.csv"))


```

Lets drop the missing column name and filter for the indicator we are actually interested in.

```{r, echo=T}

wdi <- readr::read_csv(here("01_input_data", "Worldbank", "World_Development_Indicators", "WDIData.csv")) %>%
  filter(`Indicator Code` %in% indicators) %>%
  select(-...66)

```

As you can see the dataset is in wide format. So we have to make it "tidy" first. This means we need a column for the year variable.

```{r, echo=T}

head(wdi)

wdi_long <- wdi %>%
  # pivoting all columns starting from column 5
  # since we already filtered for our indicator above we can name the value gdp_pc
  pivot_longer(
    names_to = "year",
    values_to = "gdp_pc",
    cols = 5:ncol(wdi)
  ) %>%
  # filter for the year of the remittances matrix
  filter(year == 2017) %>%
  # Drop unwanted columns
  select("Country Name", "gdp_pc") %>%
  # Drop NAs

  filter(!is.na(gdp_pc))

head(wdi_long)
```

Lets check how many Country Names in the Remittances Matrix are missing in the World Development Indicators:

```{r, echo=T}

missing_countries <- cleaned_remittances %>%
  # the anti join only keeps the variables in the remittances matrix that are not contained in the WDI dataset
  anti_join(wdi_long, by = c("name_o" = "Country Name")) %>%
  # if there are no grouping variables, the summarise output will have a single row summarising 
  # all observations in the input.
  # In our case we want the unique values of the name_o variable
  summarise(missing = unique(name_o))


missing_countries

```

As you can see there are 22 countries missing. Many of those countries are small or in which the GDP data would be pretty questionable anyways. Therefore we will exclude them from our remittances matrix before we join the data.

```{r, echo=T}

cleaned_remittance_dataset <- cleaned_remittances %>%
  
  # drop the missing countries
  filter(! name_o %in% missing_countries$missing) %>%
  filter(! name_d %in% missing_countries$missing) %>%
  
  # join the gdp data by the origin variable and rename it
  left_join(wdi_long, by=c("name_o" = "Country Name")) %>%
  rename(gdp_o = gdp_pc) %>%
  
  # join the gdp data by the destination variable and rename it
  left_join(wdi_long, by=c("name_d" = "Country Name")) %>%
  rename(gdp_d = gdp_pc)



head(cleaned_remittance_dataset)


```

### Reading in and joining the distance dataset

Now lets load in the distances dataset from CEPII. As you can see there are no country names in the dataset but 3-letter Iso country codes. We have to translate them somehow.

```{r, echo=T}
distances <- readxl::read_excel(here("01_Data_Wrangling", "01_input_data", "CEPII", "dist_cepii.xls"))

head(distances)

```

A handy solution for this hassle is the countrycode package. It is basically a collection of tables for all kinds of country codes and names.

```{r, echo = T}

cleaned_remittance_dataset <- cleaned_remittances %>%
  # drop the missing countries
  filter(!name_o %in% missing_countries$missing) %>%
  filter(!name_d %in% missing_countries$missing) %>%
  # join the gdp data by the origin variable and rename it
  left_join(wdi_long, by = c("name_o" = "Country Name")) %>%
  rename(gdp_o = gdp_pc) %>%
  # join the gdp data by the destination variable and rename it
  left_join(wdi_long, by = c("name_d" = "Country Name")) %>%
  rename(gdp_d = gdp_pc) %>%
  # lets create the iso3 country codes with the country code package
  mutate(
    iso_o = countrycode(name_o,
      origin = "country.name",
      destination = "iso3c",
      custom_match = c("Kosovo" = "KSV")
    ),
    iso_d = countrycode(name_d,
      origin = "country.name",
      destination = "iso3c",
      custom_match = c("Kosovo" = "KSV")
    )
  ) %>%
  # now join the dataset with the distance dataset
  left_join(distances, by = c("iso_o" = "iso_o", "iso_d" = "iso_d")) %>%
  # lets drop the missing values

  filter(!is.na(dist))

```

### Lets test the hypothesis

Okay we can now test our hypothesis:

```{r}

lm(log(remit + 1) ~ log(gdp_o) + log(gdp_d) + log(dist) + 
               contig + colony + comlang_off, 
   data= cleaned_remittance_dataset) %>%
  summary()


```

While the coefficients for log(gdp_o) and log(gdp_d) are statistically significant, indicating that the GDP of both the origin and destination countries have an effect on remittances, the actual magnitudes of these effects are relatively small. Specifically, a 1% increase in the GDP of the origin or destination country results in only a 0.1747% and 0.0420% change in remittances, respectively.

In the context of predictive analysis, especially when forecasting remittances or making policy decisions, such small effects might not lead to substantial changes in the predicted values. This means that while these variables contribute to the explanatory power of the model, their practical significance in terms of predicting remittances might be limited. It's essential to consider not just the statistical significance but also the practical or real-world significance of variables when making predictions or policy recommendations.

***(log + 1) Transformation:*** The transformation of log(x+1) is particularly useful in situations where data might contain zero values. Since the logarithm of zero is undefined, adding 1 to each value in the dataset ensures that we can take the logarithm of all values, including zeros. This transformation not only allows for the inclusion of zero values but also helps in stabilizing variances, making the data more symmetrical, and making patterns more linear. It's especially beneficial when dealing with skewed data, as it can make the distribution more normal-like, which can be a useful property when applying linear regression.

# Split Apply Combine

One of the most common operations in data wrangling is the so called "split apply combine" strategy. It subsets the data according to a factor variable and applies a function to each subset. You can apply all kinds of functions to the subsets, even your own, but it is usually necessary that these functions take a vector (or matrix) as argument and result in a scalar (note that the result could also be a list object like a model but this is beyond the scope of this course).


```{r, echo=FALSE, out.width="60%"}
knitr::include_graphics("./01_input_data/rmarkdown_files/art/split_apply_combine.png")
```

## mean()

### Average Temperatures per Month

The dplyr syntax for this operation is group_by (equivalent to split) and summarise (equivalent to apply).

Remember the weather dataset from above? Lets try to find the average tempearature per month.

Note that one mean is missing in the result. This is because R does not like NAs

```{r echo=TRUE, message=FALSE, warning=FALSE}

weather %>%
  # Split
  group_by(month) %>%
  # Apply mean function and combine to variable average_temp
  summarise(average_temp = mean(temp))


# Lets omit the missing tempearture in August


mean_temps <- weather %>%
  # Split
  group_by(month) %>%
  # Apply  mean function and combine to variable average_temp
  summarise(average_temp = mean(temp, na.rm = T))


mean_temps
  

```

This operation should be **VERY** familiar to you since you all had several econometrics classes by now. We simply estimated the conditional expectation per month (remember: in the limit the arithmetic mean converges to the expectation): $E(temperature|month)$.

This is what OLS does if you regress the dependent variable on dummy variables, although it usually includes an intercept. Lets check if OLS yields the same result. The month variable can be interpreted as "dummy variable" but we have to transform it to a factor variable first so that R can do it's thing under the hood. If we omit the intercept in the estimation the regression coefficients should be exactly the same.

It's important to note that in typical regression analyses, it's generally not advisable to omit the intercept, as doing so can lead to biased estimates. The intercept captures the mean of the dependent variable when all predictors are set to zero. Omitting it forces the regression line to pass through the origin, which might not be a true representation of the data. However, for the sake of this example and to demonstrate the equivalence between the OLS estimates and the group means, we'll proceed without an intercept. Always exercise caution and consider the implications when making such decisions in real-world analyses.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Lets first omit all the Missing values in the temperature variable and create a factor variable from

OLS_dataframe <- weather %>%
  # remove NAs
  filter(!is.na(temp)) %>%
  # lets transform the month variable into  a factor variable
  mutate(month = factor(month))


# Lets run the model WITHOUT the intercept

model <- lm(temp ~ month + 0, data = OLS_dataframe)

# Heres the model output
summary(model)


# this is what R does "under the hood" with factor variables when they are used in a formula

model %>%
  model.matrix() %>%
  head()

```

Yay, the math worked... And thats a nice summary and all but not terribly useful for further wrangling steps. We could now scroll up and check if the coefficients are the same as the conditional means we calculated. But lets be a bit more thorough and do it in a "data wrangling" manner. This is of course only a toy example but this workflow is super useful if you have to create tables or access your estimation coefficients for further wrangling and calculations.

The broom library has some awesome features to work with model outputs. You should use it often to make your life easier:

```{r echo=TRUE, message=FALSE, warning=FALSE}

model_estimation_outputs <- model %>%
  tidy()

model_estimation_outputs


```

We now have a data frame containing all the interesting estimation results. How would we now check if the results are the same?

Lets

-   join the two datasets together and
-   calculate the difference of the two estimations,

it should be zero if they are the same.

We don't have the same keys in the datasets. So we have to create them.

```{r echo=TRUE, message=FALSE, warning=FALSE}


mean_temps <- weather %>%
  #Split
  group_by(month) %>% 
  #Apply mean function and combine to variable average_temp
  summarise(average_temp = mean(temp, na.rm=T)) %>%
  #Apply and combine to variable average_temp
  mutate(month = paste0("month",month)) %>%
  #Join the OLS coefficients to the table
  left_join(model_estimation_outputs, by= c("month"="term")) %>%
  #Now calculate the difference between 
  # the split-apply-combine calcualtion and the OLS coefficients
  mutate(difference = average_temp - estimate,
         ratio = average_temp/estimate)


# Lets select only the relevant columns
mean_temps %>%
  select(month, average_temp, estimate, difference, ratio)

```

As you can see the estimations are (virtually) the same. The differences are due to rounding errors.

## Sum()

### Aggregating Remittances

Imagine you're an economist working for an international organization. Your task is to analyze global remittance flows. Specifically, you want to determine how much each country receives in total remittances. This information can be crucial for understanding the economic dependencies of countries on foreign incomes.

Remember the "tidy" remittances matrix? It provides details on remittance amounts sent from one country (name_o) to another (name_d).

```{r echo=TRUE, message=FALSE, warning=FALSE}


cleaned_remittances

```

To find out the total remittances received by each country, you need to aggregate or sum up all the remittances coming into that country from various origins.

```{r echo=TRUE, message=FALSE, warning=FALSE}


cleaned_remittances %>%
  # split data by name_d
  group_by(name_d) %>%
  # apply the sum function to each subset and combine the data
  summarise(total_remit = sum(remit))

```


# References

	•	Wickham, H., & Grolemund, G. (2017). R for Data Science. O’Reilly Media.
	•	Wickham, H. (2019). Advanced R, 2nd Edition.

